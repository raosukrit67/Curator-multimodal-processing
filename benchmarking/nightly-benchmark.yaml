# These two paths must be defined and are used to resolve the results and
# datasets paths. These must be existing paths on the host.
# If running inside a Docker container started with tools/run.sh, the paths
# will automatically be mapped to the appropriate volume mount.
# If running on the host, the paths will remain as defined below.
# Paths can be used in other values in this file by using their placeholder
# (e.g. {datasets_path}/my/test/dataset.parquet) and will be resolved to the
# appropriate path at runtime.
results_path: /path/where/results/are/stored
datasets_path: /path/to/datasets
model_weights_path: /path/to/model_weights

datasets:
  - name: "tinystories"
    formats:
    - type: "parquet"
      path: "{datasets_path}/tinystories/parquet_data"
    - type: "jsonl"
      path: "{datasets_path}/tinystories/jsonl_data"
  - name: "commoncrawl"
    formats:
    - type: "jsonl"
      path: "{datasets_path}/commoncrawl/jsonl_data"
    - type: "parquet_gemma_embeddings"
      path: "{datasets_path}/cleaned_exact_dedup_all_cc_gemma_embeddings"
  - name: "commoncrawl_id_map"
    formats:
    - type: "json"
      path: "{datasets_path}/commoncrawl/id_generator.json"
  - name: "commoncrawl_ids"
    formats:
    - type: "parquet"
      path: "{datasets_path}/commoncrawl/IDs/parquet_data"
  - name: "mscoco"
    formats:
    - type: "wds"
      path: "{datasets_path}/mscoco/wds/truncated_100K_mscoco_benchmarking"
  - name: "mscoco_model_weights"
    formats:
    - type: "files"
      path: "{datasets_path}/mscoco/model_weights"
  - name: "videos"
    formats:
    - type: "mp4"
      path: "{datasets_path}/videos"
  - name: "videos_model_weights"
    formats:
    - type: "files"
      path: "{datasets_path}/video_model_weights"
  - name: "rpv2-2023-14-en"
    formats:
    - type: "parquet"
      path: "{datasets_path}/rpv2_2023-14_en"
  - name: "arxiv_downloads"
    formats:
    - type: "tar"
      path: "{datasets_path}/arxiv_downloads"
  - name: "fasttext_langid_model"
    formats:
    - type: "bin"
      path: "{model_weights_path}/fasttext/lid.176.bin"
    - type: "ftz"
      path: "{model_weights_path}/fasttext/lid.176.ftz"
  - name: "fasttext_quality_model"
    formats:
    - type: "bin"
      path: "{model_weights_path}/fasttext/model.bin"
  - name: "gretel_symptoms"
    formats:
    - type: "jsonl"
      path: "{datasets_path}/gretel_symptoms"
default_timeout_s: 7200

# Optional sinks
sinks:
#  - name: mlflow
#    enabled: false
#    tracking_uri: ${MLFLOW_TRACKING_URI}
#    experiment: ray-curator-common-crawl
  - name: slack
    enabled: true
    live_updates: true
    channel_id: ${SLACK_CHANNEL_ID}
    default_metrics: ["exec_time_s"]
#  - name: gdrive
#    enabled: false
#    drive_folder_id: ${GDRIVE_FOLDER_ID}
#    service_account_file: ${GDRIVE_SERVICE_ACCOUNT_FILE}

# Whether to delete scratch dirs after each run
delete_scratch: true

# The size of the object store used by Ray which can be either a value in bytes (int), or
# a fraction of total system memory (float), or the value "default" (string) which allows
# for "ray start" to determine object store size.
object_store_size: 536870912000  # 500GB

entries:
  - name: domain_classification_raydata
    enabled: true
    script: domain_classification_benchmark.py
    args: >-
      --benchmark-results-path={session_entry_dir}
      --output-path={session_entry_dir}/scratch/output
      --executor=ray_data
      --input-path={dataset:tinystories,parquet}
      --dataset-size-gb=10
      --model-inference-batch-size=1024
    timeout_s: 1000
    sink_data:
      - name: slack
        # Additional metrics to include in the Slack report.  These must be present in the metrics.json file generated by the script.
        additional_metrics:
          - throughput_docs_per_sec
          - number_of_domains_predicted
    ray:
      num_cpus: 64
      num_gpus: 4
      enable_object_spilling: false
    requirements:
      # Observed throughput of 2700 docs/sec so we allow a 5% buffer to account for variability
      - metric: throughput_docs_per_sec
        min_value: 2565
      - metric: number_of_domains_predicted
        exact_value: 26
      # We choose two domains randomly and ensure that the counts are as expected
      - metric: domain_label_games_count
        exact_value: 149816
      - metric: domain_label_news_count
        exact_value: 2817

  - name: domain_classification_xenna
    enabled: true
    script: domain_classification_benchmark.py
    args: >-
      --benchmark-results-path={session_entry_dir}
      --output-path={session_entry_dir}/scratch/output
      --executor=xenna
      --input-path={dataset:tinystories,parquet}
      --dataset-size-gb=10
      --model-inference-batch-size=1024
    timeout_s: 1000
    sink_data:
      - name: slack
        additional_metrics:
          - throughput_docs_per_sec
          - number_of_domains_predicted
    requirements:
      # Observed throughput of 2900 docs/sec so we allow a 5% buffer to account for variability
      - metric: throughput_docs_per_sec
        min_value: 2755
      - metric: number_of_domains_predicted
        exact_value: 26
      # We choose two domains randomly and ensure that the counts are as expected
      - metric: domain_label_games_count
        exact_value: 149816
      - metric: domain_label_news_count
        exact_value: 2817

  - name: embedding_generation_raydata
    enabled: true
    script: embedding_generation_benchmark.py
    args: >-
      --benchmark-results-path={session_entry_dir}
      --output-path={session_entry_dir}/scratch/output
      --executor=ray_data
      --input-path={dataset:tinystories,parquet}
      --dataset-size-gb=10
      --model-identifier=sentence-transformers/all-MiniLM-L6-v2
      --model-inference-batch-size=1024
    timeout_s: 350
    sink_data:
      - name: slack
        additional_metrics:
          - throughput_docs_per_sec
    ray:
      num_cpus: 64
      num_gpus: 4
      enable_object_spilling: false
    requirements:
      # observed throughput of 7200 docs/sec so we allow a 5% buffer to account for variability
      - metric: throughput_docs_per_sec
        min_value: 6840
  - name: embedding_generation_xenna
    enabled: true
    script: embedding_generation_benchmark.py
    args: >-
      --benchmark-results-path={session_entry_dir}
      --output-path={session_entry_dir}/scratch/output
      --executor=xenna
      --input-path={dataset:tinystories,parquet}
      --dataset-size-gb=10
      --model-identifier=sentence-transformers/all-MiniLM-L6-v2
      --model-inference-batch-size=1024
    timeout_s: 350
    sink_data:
      - name: slack
        additional_metrics:
          - num_documents_processed
          - throughput_docs_per_sec
    requirements:
      # Observed throughput of 8600 docs/sec so we allow a 5% buffer to account for variability
      - metric: throughput_docs_per_sec
        min_value: 8170

  - name: exact_dedup_identification
    enabled: true
    script: exact_dedup_identification_benchmark.py
    args: >-
      --benchmark-results-path={session_entry_dir}
      --input-path={dataset:rpv2-2023-14-en,parquet}
      --output-path={session_entry_dir}/scratch/output
      --input-filetype=parquet
      --text-field=raw_content
      --input-blocksize=3GiB
    timeout_s: 500
    sink_data:
      - name: slack
        additional_metrics:
          - "num_duplicates"
    ray:
      num_cpus: 64
      num_gpus: 4
      enable_object_spilling: false
    requirements:
      - metric: num_duplicates
        exact_value: 64292856

  - name: fuzzy_dedup_identification
    enabled: true
    script: fuzzy_dedup_identification_benchmark.py
    args: >-
      --benchmark-results-path={session_entry_dir}
      --input-path={dataset:commoncrawl,jsonl}
      --cache-path={session_entry_dir}/scratch/cache
      --output-path={session_entry_dir}/scratch/output
      --input-filetype=jsonl
      --bands-per-iteration=20
      --text-field=text
      --input-blocksize=1.5GiB
    timeout_s: 700
    sink_data:
      - name: slack
        additional_metrics:
          - num_duplicates
          - minhash_percent_time
          - lsh_percent_time
          - connected_components_percent_time
    ray:
      num_cpus: 64
      num_gpus: 4
      enable_object_spilling: false
    requirements:
      - metric: num_duplicates
        # 59,226,133 is expected, ensure +/- 1%
        min_value: 58633871
        max_value: 59818394

  - name: semdedup_identification_xenna
    enabled: true
    script: semdedup_identification_benchmark.py
    args: >-
      --benchmark-results-path={session_entry_dir}
      --input-path={dataset:commoncrawl,parquet_gemma_embeddings}
      --cache-path={session_entry_dir}/scratch/cache
      --output-path={session_entry_dir}/scratch/output
      --executor=xenna
      --n-clusters=1000
      --id-field=_curator_dedup_id
      --embedding-field=embeddings
      --embedding-dim=1536
      --input-filetype=parquet
      --eps=0.01
      --which-to-keep=hard
      --pairwise-batch-size=1024
      --dataset-size-ratio=0.10
    timeout_s: 450
    sink_data:
      - name: slack
        additional_metrics:
          - num_documents_processed
          - num_duplicates
          # %percent time taken
          - kmeans_read_percent_time
          - kmeans_write_percent_time
          - kmeans_fit_predict_percent_time
          - pairwise_percent_time
    ray:
      num_cpus: 64
      num_gpus: 4
      enable_object_spilling: false
    requirements:
      - metric: num_documents_processed
        exact_value: 312458951
      - metric: num_duplicates
        # 494,297 is expected, ensure +/- 1%
        min_value: 489354
        max_value: 499240

  - name: dedup_removal_raydata
    enabled: true
    script: dedup_removal_benchmark.py
    args: >-
      --benchmark-results-path={session_entry_dir}
      --input-path={dataset:commoncrawl,jsonl}
      --id-generator-path={dataset:commoncrawl_id_map,json}
      --ids-to-remove-path={dataset:commoncrawl_ids,parquet}
      --output-path={session_entry_dir}/scratch/output
      --executor=ray_data
      --input-filetype=jsonl
      --output-filetype=parquet
      --id-field=_curator_dedup_id
      --duplicate-id-field=_curator_dedup_id
      --blocksize=1.5GiB
    timeout_s: 1100
    sink_data:
      - name: slack
        additional_metrics:
          - num_duplicates_removed
    ray:
      num_cpus: 64
      num_gpus: 0
      enable_object_spilling: false
    requirements:
      - metric: num_duplicates_removed
        exact_value: 59226133

  - name: dedup_removal_xenna
    enabled: true
    script: dedup_removal_benchmark.py
    args: >-
      --benchmark-results-path={session_entry_dir}
      --input-path={dataset:commoncrawl,jsonl}
      --id-generator-path={dataset:commoncrawl_id_map,json}
      --ids-to-remove-path={dataset:commoncrawl_ids,parquet}
      --output-path={session_entry_dir}/scratch/output
      --executor=xenna
      --input-filetype=jsonl
      --output-filetype=parquet
      --id-field=_curator_dedup_id
      --duplicate-id-field=_curator_dedup_id
      --blocksize=1.5GiB
    timeout_s: 1000
    sink_data:
      - name: slack
        additional_metrics:
          - num_duplicates_removed
          - io_percentage
    ray:
      num_cpus: 64
      num_gpus: 0
      enable_object_spilling: false
    requirements:
      - metric: num_duplicates_removed
        exact_value: 59226133
      - metric: io_percentage
        min_value: 97

  - name: score_filter_raydata
    enabled: true
    script: score_filter_benchmark.py
    args: >-
      --benchmark-results-path={session_entry_dir}
      --output-path={session_entry_dir}/scratch/output
      --executor=ray_data
      --input-path={dataset:tinystories,parquet}
      --yaml-config={curator_repo_dir}/nemo_curator/config/text/heuristic_filter_english_pipeline.yaml
      --overrides="stages.0._target_=nemo_curator.stages.text.io.reader.ParquetReader"
    timeout_s: 200
    sink_data:
      - name: slack
        additional_metrics:
          - num_kept_documents
          - throughput_docs_per_sec
    ray:
      num_cpus: 64
      num_gpus: 0
      enable_object_spilling: false
    requirements:
      # ensure the total number of documents processed is correct
      - metric: num_documents_processed
        exact_value: 2119489
    # account for stochastic filters
      - metric: num_kept_documents
        min_value: 2090470
        max_value: 2090490
      - metric: throughput_docs_per_sec
        min_value: 19000

  - name: score_filter_xenna
    enabled: true
    script: score_filter_benchmark.py
    args: >-
      --benchmark-results-path={session_entry_dir}
      --output-path={session_entry_dir}/scratch/output
      --executor=xenna
      --input-path={dataset:tinystories,parquet}
      --yaml-config={curator_repo_dir}/nemo_curator/config/text/heuristic_filter_english_pipeline.yaml
      --overrides="stages.0._target_=nemo_curator.stages.text.io.reader.ParquetReader"
    timeout_s: 350
    sink_data:
      - name: slack
        additional_metrics:
          - num_kept_documents
          - throughput_docs_per_sec
    ray:
      num_cpus: 64
      num_gpus: 0
      enable_object_spilling: false
    requirements:
      # ensure the total number of documents processed is correct
      - metric: num_documents_processed
        exact_value: 2119489
    # account for stochastic filters
      - metric: num_kept_documents
        min_value: 2090470
        max_value: 2090490
      - metric: throughput_docs_per_sec
        min_value: 8500

  - name: fasttext_filter_raydata
    enabled: true
    script: fasttext_filter_benchmark.py
    args: >-
      --benchmark-results-path={session_entry_dir}
      --output-path={session_entry_dir}/scratch/output
      --executor=ray_data
      --input-path={dataset:tinystories,parquet}
      --yaml-config={curator_repo_dir}/nemo_curator/config/text/fasttext_filter_pipeline.yaml
      --fasttext-langid-model-path={dataset:fasttext_langid_model,bin}
      --fasttext-quality-model-path={dataset:fasttext_quality_model,bin}
      --overrides="stages.0._target_=nemo_curator.stages.text.io.reader.ParquetReader"
    timeout_s: 200
    sink_data:
      - name: slack
        additional_metrics:
          - num_kept_documents
          - throughput_docs_per_sec
    ray:
      num_cpus: 64
      num_gpus: 0
      enable_object_spilling: false

  - name: fasttext_filter_xenna
    enabled: true
    script: fasttext_filter_benchmark.py
    args: >-
      --benchmark-results-path={session_entry_dir}
      --output-path={session_entry_dir}/scratch/output
      --executor=xenna
      --input-path={dataset:tinystories,parquet}
      --yaml-config={curator_repo_dir}/nemo_curator/config/text/fasttext_filter_pipeline.yaml
      --fasttext-langid-model-path={dataset:fasttext_langid_model,bin}
      --fasttext-quality-model-path={dataset:fasttext_quality_model,bin}
      --overrides="stages.0._target_=nemo_curator.stages.text.io.reader.ParquetReader"
    timeout_s: 100
    sink_data:
      - name: slack
        additional_metrics:
          - num_kept_documents
          - throughput_docs_per_sec
    ray:
      num_cpus: 64
      num_gpus: 0
      enable_object_spilling: false

  - name: modifier_raydata
    enabled: true
    script: modifier_benchmark.py
    args: >-
      --benchmark-results-path={session_entry_dir}
      --output-path={session_entry_dir}/scratch/output
      --executor=ray_data
      --input-path={dataset:tinystories,parquet}
    timeout_s: 200
    sink_data:
      - name: slack
        additional_metrics:
          - throughput_docs_per_sec
    ray:
      num_cpus: 64
      num_gpus: 0
      enable_object_spilling: false

  - name: modifier_xenna
    enabled: true
    script: modifier_benchmark.py
    args: >-
      --benchmark-results-path={session_entry_dir}
      --output-path={session_entry_dir}/scratch/output
      --executor=xenna
      --input-path={dataset:tinystories,parquet}
    timeout_s: 250
    sink_data:
      - name: slack
        additional_metrics:
          - throughput_docs_per_sec
    ray:
      num_cpus: 64
      num_gpus: 0
      enable_object_spilling: false

  - name: image_curation
    enabled: true
    script: image_pipeline_benchmark.py
    args: >-
      --benchmark-results-path={session_entry_dir}
      --input-wds-dataset-dir={dataset:mscoco,wds}
      --output-dataset-dir={session_entry_dir}/scratch/output
      --model-dir={dataset:mscoco_model_weights,files}
      --executor=xenna
      --tar-files-per-partition=1
      --batch-size=1000
      --embedding-batch-size=500
      --aesthetic-batch-size=500
      --aesthetic-threshold=0.9
      --verbose
    timeout_s: 1500
    ray:
      num_cpus: 64
      num_gpus: 4
      enable_object_spilling: false
    requirements:
      # ensure the total number of documents processed is correct
      - metric: num_images_processed
        exact_value: 3800
      - metric: throughput_images_per_sec
        min_value: 3.0

  - name: audio_fleurs
    enabled: true
    script: audio_fleurs_benchmark.py
    args: >-
      --benchmark-results-path={session_entry_dir}
      --scratch-output-path={session_entry_dir}/scratch
      --model-name=nvidia/stt_hy_fastconformer_hybrid_large_pc
      --lang=hy_am
      --split=dev
      --wer-threshold=5.5
      --gpus=1

  - name: arxiv_e2e_pipeline_raydata
    enabled: true
    script: arxiv_e2e_pipeline_benchmark.py
    args: >-
      --benchmark-results-path={session_entry_dir}
      --tar-input-path={dataset:arxiv_downloads,tar}
      --output-path={session_entry_dir}/scratch/output
      --fasttext-langid-model-path={dataset:fasttext_langid_model,bin}
      --executor=ray_data
    timeout_s: 3600
    sink_data:
      - name: slack
        additional_metrics:
          - throughput_docs_per_sec
          - num_output_documents
          - num_input_documents
    ray:
      num_cpus: 16
      num_gpus: 4
      enable_object_spilling: false
    requirements:
      # Data integrity checks
      - metric: num_tar_files
        exact_value: 45
      - metric: num_input_documents
        exact_value: 116383
      - metric: num_output_documents
        exact_value: 61474

  - name: arxiv_e2e_pipeline_xenna
    enabled: true
    script: arxiv_e2e_pipeline_benchmark.py
    args: >-
      --benchmark-results-path={session_entry_dir}
      --tar-input-path={dataset:arxiv_downloads,tar}
      --output-path={session_entry_dir}/scratch/output
      --fasttext-langid-model-path={dataset:fasttext_langid_model,bin}
      --executor=xenna
    timeout_s: 3600
    sink_data:
      - name: slack
        additional_metrics:
          - throughput_docs_per_sec
          - num_output_documents
          - num_input_documents
    ray:
      num_cpus: 16
      num_gpus: 4
      enable_object_spilling: false
    requirements:
      # Data integrity checks
      - metric: num_tar_files
        exact_value: 45
      - metric: num_input_documents
        exact_value: 116383
      - metric: num_output_documents
        exact_value: 61474
  - name: video_embedding
    enabled: true
    script: video_pipeline_benchmark.py
    args: >-
      --benchmark-results-path={session_entry_dir}
      --output-path={session_entry_dir}/scratch/output
      --executor=xenna
      --video-dir={dataset:videos,mp4}
      --model-dir={dataset:videos_model_weights,files}
      --video-limit=1000
    timeout_s: 400
    ray:
      num_cpus: 64
      num_gpus: 4
      enable_object_spilling: false
    requirements:
      # ensure the total number of documents processed is correct
      - metric: num_clips_generated
        exact_value: 1400
      - metric: throughput_clips_per_sec
        min_value: 4.0

  - name: video_transcoding
    enabled: true
    script: video_pipeline_benchmark.py
    args: >-
      --benchmark-results-path={session_entry_dir}
      --output-path={session_entry_dir}/scratch/output
      --executor=xenna
      --no-generate-embeddings
      --video-dir={dataset:videos,mp4}
      --video-limit=1000
    timeout_s: 400
    ray:
      num_cpus: 64
      num_gpus: 0
      enable_object_spilling: false
    requirements:
      # ensure the total number of documents processed is correct
      - metric: num_clips_generated
        exact_value: 1400
      - metric: throughput_clips_per_sec
        min_value: 5.0

  - name: video_captioning
    enabled: true
    script: video_pipeline_benchmark.py
    args: >-
      --benchmark-results-path={session_entry_dir}
      --output-path={session_entry_dir}/scratch/output
      --executor=xenna
      --video-dir={dataset:videos,mp4}
      --model-dir={dataset:videos_model_weights,files}
      --generate-captions
      --no-generate-embeddings
      --enhance-captions
      --video-limit=256
    timeout_s: 1800
    ray:
      num_cpus: 64
      num_gpus: 4
      enable_object_spilling: false
    requirements:
      # ensure the total number of documents processed is correct
      - metric: num_clips_generated
        exact_value: 300 # TODO: update this value after benchmarking
      - metric: throughput_clips_per_sec
        min_value: 0.25

  - name: ndd_nvidia_nim
    enabled: true
    script: ndd_benchmark.py
    args: >-
      --benchmark-results-path={session_entry_dir}
      --input-path={dataset:gretel_symptoms,jsonl}
      --output-path={session_entry_dir}/scratch/output
      --model-type=nvidia-nim
      --model-id=openai/gpt-oss-20b
      --executor=ray_data
    timeout_s: 3600
    ray:
      num_cpus: 8
      num_gpus: 0
      enable_object_spilling: false
    sink_data:
      - name: slack
        additional_metrics:
          - input_row_count
          - output_row_count
          - throughput_rows_per_sec
    requirements:
      - metric: output_row_count
        exact_value: 820

  - name: video_transnetv2_motion_aesthetic_filter_embeddings
    enabled: true
    script: video_pipeline_benchmark.py
    args: >-
      --benchmark-results-path={session_entry_dir}
      --output-path={session_entry_dir}/scratch/output
      --executor=xenna
      --video-dir={dataset:videos,mp4}
      --model-dir={dataset:videos_model_weights,files}
      --splitting-algorithm=transnetv2
      --motion-filter=enable
      --aesthetic-threshold=3.5
      --transnetv2-frame-decoder-mode ffmpeg_cpu
      --video-limit=1000
    timeout_s: 500
    ray:
      num_cpus: 64
      num_gpus: 4
      enable_object_spilling: false
    requirements:
      # ensure the total number of documents processed is correct
      - metric: num_clips_generated
        exact_value: 113
      - metric: throughput_clips_per_sec
        min_value: 0.25
