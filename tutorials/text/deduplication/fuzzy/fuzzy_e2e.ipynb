{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe55f10a-8210-4746-9ddd-f7ad60a5cc52",
   "metadata": {},
   "source": [
    "# End-to-end fuzzy deduplication\n",
    "\n",
    "GPU accelerated implementation of a MinHash-LSH based fuzzy deduplication. For more information about fuzzy deduplication in NeMo Curator, refer to the [Deduplication](https://docs.nvidia.com/nemo/curator/latest/curate-text/process-data/deduplication/index.html) section of the documentation page.\n",
    "\n",
    "The tutorial here shows how to run fuzzy deduplication on text data by executing 2 end to end workflows.\n",
    "These 2 workflows roughly cover the following steps to perform fuzzy deduplication:\n",
    "\n",
    "1. Read original dataset\n",
    "2. Compute MinHashes signatures of these documents\n",
    "3. Perform LSH - Group Minhashes into bands/buckets and shuffle these bands/buckets so that documents in the same bucket are in the same batch/file.\n",
    "4. Convert the LSH outputs (bucket_id -> doc_id mapping) into an edgelist in preparation for connected components. \n",
    "5. Compute connected components across all potential duplicates found via LSH.\n",
    "6. Generate list of duplicate documents by randomly selecting 1 document to keep from each group/component and dropping the rest.\n",
    "7. Remove duplicates based on the generated duplicate list.\n",
    "\n",
    "We also allow users to also run these steps independently, which will be covered in the step by step tutorial in the same directory as this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c4a272b-4ea6-4b03-9f57-be4cd16812cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import fsspec\n",
    "\n",
    "# Silence Curator logs via Loguru\n",
    "os.environ[\"LOGURU_LEVEL\"] = \"ERROR\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "input_dataset_path = \"./input\"  # Path to input dataset\n",
    "fuzzy_output_dir = \"./fuzzy_outputs\"  # Path to store all fuzzy outputs including cache & deduped dataset\n",
    "\n",
    "fuzzy_cache_path = os.path.join(\n",
    "    fuzzy_output_dir, \"cache\"\n",
    ")  # Path to store fuzzy deduplication intermediates (minhash, lsh etc.)\n",
    "deduplicated_output_path = os.path.join(fuzzy_output_dir, \"fuzzy_deduped_dataset\")\n",
    "\n",
    "input_filetype = (\n",
    "    \"parquet\"  # this can be either of jsonl or parquet (you'll need to change how input data is generated)\n",
    ")\n",
    "# Note: It's important that this is constant across identification and removal.\n",
    "# More information about choosing a good blocksize is mentioned in the performance considerations section below\n",
    "input_blocksize = \"512MiB\"\n",
    "output_filetype = \"parquet\"  # this can be either of jsonl or parquet\n",
    "\n",
    "storage_options = None  # Optional additional cloud I/O args to pass into Pandas/cuDF during I/O operations.\n",
    "io_kwargs = {\"storage_options\": storage_options} if storage_options is not None else None\n",
    "fs, _ = fsspec.url_to_fs(fuzzy_cache_path, **storage_options if storage_options is not None else {})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3427f692-3e90-4ff8-9f90-a18f071182d6",
   "metadata": {},
   "source": [
    "### Downloading and saving a sample dataset\n",
    "\n",
    "We download and save the [Tinystories](https://huggingface.co/datasets/roneneldan/TinyStories) dataset to the specified `input_dataset_path` above. This step can be skipped if running on a different dataset that's already present in the input_dataset_path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6f399d1-2486-4b8d-bd40-d3ddbd77dbaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fe4e4f8f5554404b3d49018d3957c04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb1218fbca69459b808d09b0dde87c03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00004-2d5a1467fff108(…):   0%|          | 0.00/249M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bd62c97bbe74a438991b5db9c8e22cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00001-of-00004-5852b56a2bd28f(…):   0%|          | 0.00/248M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4c639f5b3c940b593d65174964fd387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00002-of-00004-a26307300439e9(…):   0%|          | 0.00/246M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfd7dd44f2124b5aae5007a01a2e0b03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00003-of-00004-d243063613e5a0(…):   0%|          | 0.00/248M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4ad0db207854f99996a6dc0473c8345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/validation-00000-of-00001-869c898b5(…):   0%|          | 0.00/9.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46741aa5faa14877a55353a4385ca3cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2119719 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce25c33bee0946e29e34a4b3950a05c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/21990 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 0\n",
      "Processing file 50\n",
      "Processing file 100\n",
      "Processing file 150\n",
      "Processing file 200\n",
      "Created 212 files\n"
     ]
    }
   ],
   "source": [
    "from nemo_curator.utils.file_utils import get_all_file_paths_under\n",
    "\n",
    "if len(get_all_file_paths_under(input_dataset_path, storage_options=storage_options)) == 0:\n",
    "    import os\n",
    "    import uuid\n",
    "\n",
    "    from datasets import load_dataset\n",
    "\n",
    "    input_df = load_dataset(\"roneneldan/TinyStories\", split=\"train\").to_pandas()\n",
    "    num_rows_per_file = 10_000\n",
    "\n",
    "    os.makedirs(input_dataset_path, exist_ok=True)\n",
    "\n",
    "    for i, start_idx in enumerate(range(0, len(input_df), num_rows_per_file)):\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Processing file {i}\")\n",
    "        end_idx = min(len(input_df), start_idx + num_rows_per_file)\n",
    "        subset_df = input_df.iloc[start_idx:end_idx].copy()\n",
    "        subset_df[\"id\"] = [str(uuid.uuid4()) for _ in range(len(subset_df))]\n",
    "        subset_df.to_parquet(\n",
    "            os.path.join(input_dataset_path, f\"part_{i}.parquet\"), index=False, storage_options=storage_options\n",
    "        )\n",
    "\n",
    "    print(f\"Created {i + 1} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabcb72d-6d22-4324-a51c-73241f0d9d19",
   "metadata": {},
   "source": [
    "## Running as a Single Stage (End-to-End)\n",
    "\n",
    "See the [API Reference](https://docs.nvidia.com/nemo/curator/latest/apidocs/stages/stages.deduplication.fuzzy.workflow.html#api) for more information about the `FuzzyDeduplicationWorkflow` class.\n",
    "\n",
    "### General Notes\n",
    "#### ID Generation\n",
    "1. The ID generation process requires a Ray cluster to be started before running the workflow either from the CLI or by using the `RayClient` API in Curator.\n",
    "2. The `FuzzyDeduplicationWorkflow` API doesn't utilize any existing IDs in the input dataset and instead generates IDs on the fly using an ID Generator actor.\n",
    "3. The ID Generator gives each row a unique increasing integer ID, based on the order files are read.\n",
    "4. This avoids expensive ID->Integer encoding for the underlying connected components algorithm which only supports integer IDs.\n",
    "5. When we find duplicates, we save these integer IDs in sorted files with multiple row groups.\n",
    "6. We also save a `fuzzy_id_generator.json` which maintains a mapping of input file partitions to ID ranges for that batch.\n",
    "7. During removal, reading the same file groups will give the same integer IDs, using the min/max ID values, we can find all corresponding duplicates in that range making the process faster.\n",
    "\n",
    "#### Performance Considerations\n",
    "1. LSH - Configuring `bands_per_iteration` controls how many bands to process simultaneously in a single shuffle. Higher values can lead to faster performance but might increase memory pressure.\n",
    "2. A low `input_blocksize` may not saturate the GPUs enough while a high `input_blocksize` can lead to OOM errors during MinHash and excessive object store usage during removal. It's recommend to keep it at 512MiB-1.5GiB and reduce if running into OOMs during MinHash.\n",
    "3. The removal step can be memory intensive and it's recommend to set a higher fraction of object store memory for removal (if the machine has enough RAM). The `RayDataExecutor` showed better results during duplicate removal.\n",
    "4. The removal workflow is CPU only and can be run  on machines that don't have GPUs\n",
    "\n",
    "#### Hyperparameter Considerations\n",
    "1. The current defaults for fuzzy deduplication (260 hashes, 13 hashes per band) approximate finding documents with a Jaccard similarity of 0.8. For more information on selecting the number of bands/hashes it's recommended to analyze the S curve and tolerable threshold for false positives (and negatives). More information about LSH can be found in section `3.4.2` [here](http://infolab.stanford.edu/~ullman/mmds/ch3n.pdf).\n",
    "2. The `char_ngrams` values of 24 is set to approximate roughly ngrams that correspond to ~5 words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5043205-c068-4603-9b26-43bff908253e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "\n",
    "from nemo_curator.backends.experimental.ray_data import RayDataExecutor\n",
    "from nemo_curator.core.client import RayClient\n",
    "\n",
    "NUM_GPUS = 2\n",
    "\n",
    "if torch.cuda.device_count() < NUM_GPUS:\n",
    "    error_msg = \"The number of GPUs on this machine are lesser than the default this tutorial was tested with, please update `num_gpus` passed into `RayClient`\"\n",
    "    raise ValueError(error_msg)\n",
    "\n",
    "client = RayClient(num_cpus=64, num_gpus=NUM_GPUS)  # change as needed\n",
    "client.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1ed4970-955a-4ad1-be38-d2eda9b2dc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.stages.deduplication.fuzzy import FuzzyDeduplicationWorkflow\n",
    "\n",
    "# All workflows support passing in different kwargs and storage_options for the read, cache and output datasets\n",
    "# We use a common one here for simplicity\n",
    "\n",
    "identification_workflow = FuzzyDeduplicationWorkflow(\n",
    "    cache_path=fuzzy_cache_path,\n",
    "    output_path=fuzzy_output_dir,\n",
    "    input_path=input_dataset_path,\n",
    "    input_filetype=input_filetype,\n",
    "    input_blocksize=input_blocksize,\n",
    "    text_field=\"text\",\n",
    "    seed=42,\n",
    "    char_ngrams=24,\n",
    "    minhashes_per_band=13,\n",
    "    bands_per_iteration=10,\n",
    "    read_kwargs=io_kwargs,\n",
    "    cache_kwargs=io_kwargs,\n",
    "    write_kwargs=io_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db4931f5-5377-4b8d-9a9e-69f0e63d4ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-15 23:08:45,515\tINFO worker.py:1696 -- Using address 127.0.1.1:6380 set in the environment variable RAY_ADDRESS\n",
      "2025-12-15 23:08:45,521\tINFO worker.py:1837 -- Connecting to existing Ray cluster at address: 127.0.1.1:6380...\n",
      "[2025-12-15 23:08:45,534 W 17220 17220] global_state_accessor.cc:505: Some processes that the driver needs to connect to have not registered with GCS, so retrying. Have you run 'ray start' on this node?\n",
      "[2025-12-15 23:08:46,536 W 17220 17220] global_state_accessor.cc:505: Some processes that the driver needs to connect to have not registered with GCS, so retrying. Have you run 'ray start' on this node?\n",
      "[2025-12-15 23:08:47,538 W 17220 17220] global_state_accessor.cc:505: Some processes that the driver needs to connect to have not registered with GCS, so retrying. Have you run 'ray start' on this node?\n",
      "[2025-12-15 23:08:48,539 I 17220 17220] global_state_accessor.cc:487: This node has an IP address of 127.0.1.1, but we cannot find a local Raylet with the same address. This can happen when you connect to the Ray cluster with a different IP address or when connecting to a container.\n",
      "2025-12-15 23:08:48,543\tINFO worker.py:2023 -- Connected to Ray cluster.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-15 23:08:40,526\tINFO usage_lib.py:447 -- Usage stats collection is disabled.\n",
      "2025-12-15 23:08:40,526\tINFO scripts.py:919 -- \u001b[37mLocal node IP\u001b[39m: \u001b[1m127.0.1.1\u001b[22m\n",
      "2025-12-15 23:08:48,520\tSUCC scripts.py:963 -- \u001b[32m--------------------\u001b[39m\n",
      "2025-12-15 23:08:48,521\tSUCC scripts.py:964 -- \u001b[32mRay runtime started.\u001b[39m\n",
      "2025-12-15 23:08:48,521\tSUCC scripts.py:965 -- \u001b[32m--------------------\u001b[39m\n",
      "2025-12-15 23:08:48,521\tINFO scripts.py:967 -- \u001b[36mNext steps\u001b[39m\n",
      "2025-12-15 23:08:48,521\tINFO scripts.py:970 -- To add another node to this Ray cluster, run\n",
      "2025-12-15 23:08:48,521\tINFO scripts.py:973 -- \u001b[1m  ray start --address='127.0.1.1:6380'\u001b[22m\n",
      "2025-12-15 23:08:48,521\tINFO scripts.py:982 -- To connect to this Ray cluster:\n",
      "2025-12-15 23:08:48,521\tINFO scripts.py:984 -- \u001b[35mimport\u001b[39m\u001b[26m ray\n",
      "2025-12-15 23:08:48,521\tINFO scripts.py:985 -- ray\u001b[35m.\u001b[39m\u001b[26minit(_node_ip_address\u001b[35m=\u001b[39m\u001b[26m\u001b[33m'127.0.1.1'\u001b[39m\u001b[26m)\n",
      "2025-12-15 23:08:48,521\tINFO scripts.py:997 -- To submit a Ray job using the Ray Jobs CLI:\n",
      "2025-12-15 23:08:48,521\tINFO scripts.py:998 -- \u001b[1m  RAY_API_SERVER_ADDRESS='http://127.0.0.1:8267' ray job submit --working-dir . -- python my_script.py\u001b[22m\n",
      "2025-12-15 23:08:48,521\tINFO scripts.py:1007 -- See https://docs.ray.io/en/latest/cluster/running-applications/job-submission/index.html \n",
      "2025-12-15 23:08:48,521\tINFO scripts.py:1011 -- for more information on submitting Ray jobs to the Ray cluster.\n",
      "2025-12-15 23:08:48,521\tINFO scripts.py:1016 -- To terminate the Ray runtime, run\n",
      "2025-12-15 23:08:48,521\tINFO scripts.py:1017 -- \u001b[1m  ray stop\u001b[22m\n",
      "2025-12-15 23:08:48,521\tINFO scripts.py:1020 -- To view the status of the cluster, use\n",
      "2025-12-15 23:08:48,521\tINFO scripts.py:1021 --   \u001b[1mray status\u001b[22m\u001b[26m\n",
      "2025-12-15 23:08:48,521\tINFO scripts.py:1025 -- To monitor and debug Ray, view the dashboard at \n",
      "2025-12-15 23:08:48,521\tINFO scripts.py:1026 --   \u001b[1m127.0.0.1:8267\u001b[22m\u001b[26m\n",
      "2025-12-15 23:08:48,521\tINFO scripts.py:1033 -- \u001b[4mIf connection to the dashboard fails, check your firewall settings and network configuration.\u001b[24m\n",
      "2025-12-15 23:08:48,521\tINFO scripts.py:1139 -- \u001b[36m\u001b[1m--block\u001b[22m\u001b[39m\n",
      "2025-12-15 23:08:48,521\tINFO scripts.py:1140 -- This command will now block forever until terminated by a signal.\n",
      "2025-12-15 23:08:48,521\tINFO scripts.py:1143 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.\n",
      "2025-12-15 23:08:48,521\tINFO scripts.py:1148 -- Process exit logs will be saved to: \u001b[1m/tmp/ray/session_2025-12-15_23-08-40_527975_17737/logs/ray_process_exit.log\u001b[22m\u001b[26m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.12/site-packages/ray/_private/worker.py:2062: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
      "  warnings.warn(\n",
      "2025-12-15 23:08:51,273\tINFO worker.py:1696 -- Using address 127.0.1.1:6380 set in the environment variable RAY_ADDRESS\n",
      "2025-12-15 23:08:51,277\tINFO worker.py:1837 -- Connecting to existing Ray cluster at address: 127.0.1.1:6380...\n",
      "2025-12-15 23:08:51,284\tINFO worker.py:2014 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8267 \u001b[39m\u001b[22m\n",
      "2025-12-15 23:09:14,403\tINFO worker.py:1696 -- Using address 127.0.1.1:6380 set in the environment variable RAY_ADDRESS\n",
      "2025-12-15 23:09:14,407\tINFO worker.py:1837 -- Connecting to existing Ray cluster at address: 127.0.1.1:6380...\n",
      "2025-12-15 23:09:14,414\tINFO worker.py:2014 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8267 \u001b[39m\u001b[22m\n",
      "2025-12-15 23:09:58,710\tINFO worker.py:1696 -- Using address 127.0.1.1:6380 set in the environment variable RAY_ADDRESS\n",
      "2025-12-15 23:09:58,714\tINFO worker.py:1837 -- Connecting to existing Ray cluster at address: 127.0.1.1:6380...\n",
      "2025-12-15 23:09:58,720\tINFO worker.py:2014 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8267 \u001b[39m\u001b[22m\n",
      "2025-12-15 23:10:47,331\tINFO worker.py:1696 -- Using address 127.0.1.1:6380 set in the environment variable RAY_ADDRESS\n",
      "2025-12-15 23:10:47,336\tINFO worker.py:1837 -- Connecting to existing Ray cluster at address: 127.0.1.1:6380...\n",
      "2025-12-15 23:10:47,342\tINFO worker.py:2014 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8267 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identification workflow took: 121.86s\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "_ = identification_workflow.run()\n",
    "print(f\"Identification workflow took: {(time.time() - st):.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf7a7a3-4659-45b3-be86-5803efb54f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.stages.deduplication.id_generator import CURATOR_DEDUP_ID_STR\n",
    "from nemo_curator.stages.text.deduplication import TextDuplicatesRemovalWorkflow\n",
    "\n",
    "removal_workflow = TextDuplicatesRemovalWorkflow(\n",
    "    input_path=input_dataset_path,  # Must be identical to the path used during identification\n",
    "    ids_to_remove_path=os.path.join(fuzzy_output_dir, \"FuzzyDuplicateIds\"),\n",
    "    output_path=deduplicated_output_path,\n",
    "    input_filetype=input_filetype,\n",
    "    input_blocksize=input_blocksize,  # This must be identical to the blocksize used during identification\n",
    "    duplicate_id_field=CURATOR_DEDUP_ID_STR,\n",
    "    id_generator_path=os.path.join(fuzzy_output_dir, \"fuzzy_id_generator.json\"),\n",
    "    output_filetype=output_filetype,\n",
    "    input_kwargs=io_kwargs,  # read_kwargs for input dataset\n",
    "    duplicate_id_read_kwargs=io_kwargs,  # read_kwargs for removal_id's generated by Fuzzy workflow\n",
    "    id_generator_storage_options=storage_options,\n",
    "    output_kwargs=io_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7329ff2-3a36-42ad-833a-9589653d0e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-15 23:12:51,080\tINFO worker.py:1696 -- Using address 127.0.1.1:6380 set in the environment variable RAY_ADDRESS\n",
      "2025-12-15 23:12:51,084\tINFO worker.py:1837 -- Connecting to existing Ray cluster at address: 127.0.1.1:6380...\n",
      "2025-12-15 23:12:51,084\tINFO worker.py:1855 -- Calling ray.init() again after it has already been called.\n",
      "2025-12-15 23:12:51,317\tINFO worker.py:1696 -- Using address 127.0.1.1:6380 set in the environment variable RAY_ADDRESS\n",
      "2025-12-15 23:12:51,321\tINFO worker.py:1837 -- Connecting to existing Ray cluster at address: 127.0.1.1:6380...\n",
      "2025-12-15 23:12:51,327\tINFO worker.py:2014 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8267 \u001b[39m\u001b[22m\n",
      "2025-12-15 23:12:59,888\tWARNING util.py:598 -- The argument ``concurrency`` is deprecated in Ray 2.51. Please specify argument ``compute`` instead. For more information, see https://docs.ray.io/en/master/data/transforming-data.html#stateful-transforms.\n",
      "2025-12-15 23:12:59,895\tINFO logging.py:397 -- Registered dataset logger for dataset dataset_5_0\n",
      "2025-12-15 23:12:59,904\tINFO streaming_executor.py:174 -- Starting execution of Dataset dataset_5_0. Full logs are in /tmp/ray/session_2025-12-15_23-08-40_527975_17737/logs/ray-data\n",
      "2025-12-15 23:12:59,905\tINFO streaming_executor.py:175 -- Execution plan of Dataset dataset_5_0: InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(FilePartitioningStageTask)] -> TaskPoolMapOperator[StreamingRepartition] -> ActorPoolMapOperator[MapBatches(ParquetReaderStageActor)] -> TaskPoolMapOperator[MapBatches(TextDuplicatesRemovalStageTask)->MapBatches(ParquetWriterTask)]\n",
      "2025-12-15 23:12:59,945\tINFO streaming_executor.py:682 -- [dataset]: A new progress UI is available. To enable, set `ray.data.DataContext.get_current().enable_rich_progress_bars = True` and `ray.data.DataContext.get_current().use_ray_tqdm = False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fb7fa78ab5c44d9b3280dcfdd15fa6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87f1890cb0e242449abb96fecf714822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- MapBatches(FilePartitioningStageTask) 1: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63807253648140888cb1dc08dd250da3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- StreamingRepartition 2: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "443551e9661a4c7a8c35b695920f5f8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- MapBatches(ParquetReaderStageActor) 3: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58e29444ce9b42cfa17321dde7bd5fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- MapBatches(TextDuplicatesRemovalStageTask)->MapBatches(ParquetWriterTask) 4: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-15 23:12:59,982\tWARNING resource_manager.py:136 -- ⚠️  Ray's object store is configured to use only 10.2% of available memory (186.3GiB out of 1823.9GiB total). For optimal Ray Data performance, we recommend setting the object store to at least 50% of available memory. You can do this by setting the 'object_store_memory' parameter when calling ray.init() or by setting the RAY_DEFAULT_OBJECT_STORE_MEMORY_PROPORTION environment variable.\n",
      "2025-12-15 23:13:22,279\tINFO streaming_executor.py:300 -- ✔️  Dataset dataset_5_0 execution finished in 22.37 seconds\n",
      "2025-12-15 23:13:22,288\tINFO util.py:257 -- Exiting prefetcher's background thread\n",
      "2025-12-15 23:13:22,304\tINFO worker.py:1696 -- Using address 127.0.1.1:6380 set in the environment variable RAY_ADDRESS\n",
      "2025-12-15 23:13:22,309\tINFO worker.py:1837 -- Connecting to existing Ray cluster at address: 127.0.1.1:6380...\n",
      "2025-12-15 23:13:22,316\tINFO worker.py:2014 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8267 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removal workflow took: 31.28s\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "_ = removal_workflow.run(executor=RayDataExecutor())\n",
    "print(f\"Removal workflow took: {(time.time() - st):.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c983f3-f562-45e6-bd09-5660cb179aa3",
   "metadata": {},
   "source": [
    "### Looking at Intermediate Results and Output\n",
    "\n",
    "#### MinHash Results\n",
    "1. `_curator_dedup_id` - The IDs assigned to this dataset on the fly during the initial read.\n",
    "2. `_minhash_signature` - MinHash Signature\n",
    "\n",
    "#### LSH Results\n",
    "1. `_bucket_id` - The bucket/band identifier\n",
    "2. `_curator_dedup_id` - List of all document IDs that belong to that bucket\n",
    "\n",
    "#### Buckets To Edges Result\n",
    "1. `_curator_dedup_id_x`, `_curator_dedup_id_y` - Mapping of edges in a Graph where each column are documents that are potential duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68d9b66f-0d48-4a90-96d6-570fd218777c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_curator_dedup_id</th>\n",
       "      <th>_minhash_signature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[218051, 2965574, 2358869, 20793331, 9567445, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[13231761, 1801895, 1933976, 3402840, 8234515,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[13972691, 2206484, 3887953, 1782578, 7445153,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[5066913, 6771503, 375732, 841498, 7703292, 45...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[4066453, 951833, 9469185, 3399185, 1533452, 6...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _curator_dedup_id                                 _minhash_signature\n",
       "0                  0  [218051, 2965574, 2358869, 20793331, 9567445, ...\n",
       "1                  1  [13231761, 1801895, 1933976, 3402840, 8234515,...\n",
       "2                  2  [13972691, 2206484, 3887953, 1782578, 7445153,...\n",
       "3                  3  [5066913, 6771503, 375732, 841498, 7703292, 45...\n",
       "4                  4  [4066453, 951833, 9469185, 3399185, 1533452, 6..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_bucket_id</th>\n",
       "      <th>_curator_dedup_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b0_000055fd7daae1e46223e8b7e06bf2e0</td>\n",
       "      <td>[68375, 969489]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b0_0000f975e5bcda25838df43b0d37737f</td>\n",
       "      <td>[224885, 1975572]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b0_0001c9dff36e10d709d64123cb0dee4d</td>\n",
       "      <td>[826007, 1309488]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b0_00020b2c889483bd6a78ffe9a8d7deb1</td>\n",
       "      <td>[908278, 1270888]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b0_00024c2b7321353410dd908eb31499bd</td>\n",
       "      <td>[1222795, 2000426]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            _bucket_id   _curator_dedup_id\n",
       "0  b0_000055fd7daae1e46223e8b7e06bf2e0     [68375, 969489]\n",
       "1  b0_0000f975e5bcda25838df43b0d37737f   [224885, 1975572]\n",
       "2  b0_0001c9dff36e10d709d64123cb0dee4d   [826007, 1309488]\n",
       "3  b0_00020b2c889483bd6a78ffe9a8d7deb1   [908278, 1270888]\n",
       "4  b0_00024c2b7321353410dd908eb31499bd  [1222795, 2000426]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_curator_dedup_id_x</th>\n",
       "      <th>_curator_dedup_id_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68375</td>\n",
       "      <td>969489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>224885</td>\n",
       "      <td>1975572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>826007</td>\n",
       "      <td>1309488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>908278</td>\n",
       "      <td>1270888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1222795</td>\n",
       "      <td>2000426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _curator_dedup_id_x  _curator_dedup_id_y\n",
       "0                68375               969489\n",
       "1               224885              1975572\n",
       "2               826007              1309488\n",
       "3               908278              1270888\n",
       "4              1222795              2000426"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "minhash_path = os.path.join(fuzzy_cache_path, \"MinHashStage\")\n",
    "display(pd.read_parquet(fs.unstrip_protocol(fs.find(minhash_path)[0]), storage_options=storage_options).head())\n",
    "\n",
    "lsh_path = os.path.join(fuzzy_cache_path, \"LSHStage\")\n",
    "display(pd.read_parquet(fs.unstrip_protocol(fs.find(lsh_path)[0]), storage_options=storage_options).head())\n",
    "\n",
    "b2e_path = os.path.join(fuzzy_cache_path, \"BucketsToEdgesStage\")\n",
    "display(pd.read_parquet(fs.unstrip_protocol(fs.find(b2e_path)[0]), storage_options=storage_options).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650db74e-b764-44fa-966a-e4f0ddcb7182",
   "metadata": {},
   "source": [
    "#### Connected Components Result\n",
    "\n",
    "1. `_curator_dedup_id` - The document IDs\n",
    "2. `_duplicate_group_id` - The group ID that document belongs to. Documents with the same duplicate group ID are duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2549cfe9-3003-414c-a5cb-666e458b4615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_curator_dedup_id</th>\n",
       "      <th>_duplicate_group_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>171083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>491932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>491933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>320428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>171086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640509</th>\n",
       "      <td>2119713</td>\n",
       "      <td>132508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640510</th>\n",
       "      <td>2119714</td>\n",
       "      <td>320421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640511</th>\n",
       "      <td>2119715</td>\n",
       "      <td>320422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640512</th>\n",
       "      <td>2119716</td>\n",
       "      <td>453258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640513</th>\n",
       "      <td>2119718</td>\n",
       "      <td>320424</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>640514 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        _curator_dedup_id  _duplicate_group_id\n",
       "0                       3               171083\n",
       "1                       5               491932\n",
       "2                       6               491933\n",
       "3                       7               320428\n",
       "4                       8               171086\n",
       "...                   ...                  ...\n",
       "640509            2119713               132508\n",
       "640510            2119714               320421\n",
       "640511            2119715               320422\n",
       "640512            2119716               453258\n",
       "640513            2119718               320424\n",
       "\n",
       "[640514 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "_duplicate_group_id\n",
       "0                [603797, 0]\n",
       "1                [603798, 1]\n",
       "2                [2, 603799]\n",
       "5               [12, 603809]\n",
       "6               [603812, 15]\n",
       "                 ...        \n",
       "640502    [1237637, 2119693]\n",
       "640506    [2119701, 1237645]\n",
       "640507    [2119702, 1237646]\n",
       "640510    [2119706, 1237650]\n",
       "640511    [2119707, 1237651]\n",
       "Name: _curator_dedup_id, Length: 320043, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "_duplicate_group_id\n",
       "14100     230\n",
       "153774      3\n",
       "269755      3\n",
       "269739      3\n",
       "269745      3\n",
       "         ... \n",
       "427192      2\n",
       "198728      2\n",
       "198726      2\n",
       "213120      2\n",
       "310717      2\n",
       "Name: count, Length: 320043, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cc_path = os.path.join(fuzzy_cache_path, \"ConnectedComponentsStage\")\n",
    "cc_df = pd.read_parquet(cc_path, storage_options=storage_options)  # works with pandas since the input here is small\n",
    "display(cc_df)\n",
    "grouped_cc_df = cc_df.groupby(\"_duplicate_group_id\")._curator_dedup_id.agg(list)\n",
    "display(grouped_cc_df)\n",
    "duplicate_cluster_sizes = cc_df._duplicate_group_id.value_counts()\n",
    "display(duplicate_cluster_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b87d677-40aa-49e1-bf6d-ab41a0941957",
   "metadata": {},
   "source": [
    "Based on the distribution above we can see that there is one cluster/group where 230 documents are all duplicates followed by many smaller clusters with 2 or 3 documents that are duplicates.\n",
    "\n",
    "#### FuzzyDuplicateIds Results (List of duplicate docs to remove)\n",
    "1. `_curator_dedup_id` - ID of docs in the removal list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eda82d7a-8b35-4e64-8af0-5163bf1be875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_curator_dedup_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _curator_dedup_id\n",
       "0                 13\n",
       "1                 25\n",
       "2                 32\n",
       "3                 53\n",
       "4                 56"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate documents found for removal: 320471\n"
     ]
    }
   ],
   "source": [
    "duplicate_ids_path = os.path.join(fuzzy_output_dir, \"FuzzyDuplicateIds\")\n",
    "duplicates_df = pd.read_parquet(duplicate_ids_path, storage_options=storage_options)\n",
    "display(duplicates_df.head())\n",
    "\n",
    "print(f\"Number of duplicate documents found for removal: {len(duplicates_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af14853-2900-4d41-8792-6ff2b1cafb4c",
   "metadata": {},
   "source": [
    "#### Checking that the duplicate ids list contains only one document per group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a1f8bf4-8e1b-425c-91f1-3d312bf17786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in the duplicate group: 230\n",
      "Number of documents in the removal list from the same group: 229\n"
     ]
    }
   ],
   "source": [
    "# As an example let's look at the group with the largest number of duplicates\n",
    "largest_duplicate_cluster = grouped_cc_df.loc[duplicate_cluster_sizes.index[0]]\n",
    "\n",
    "# number of docs in the removal list from this group\n",
    "docs_to_remove_in_group = duplicates_df._curator_dedup_id.isin(largest_duplicate_cluster).sum()\n",
    "\n",
    "print(f\"Number of documents in the duplicate group: {len(largest_duplicate_cluster)}\")\n",
    "print(f\"Number of documents in the removal list from the same group: {docs_to_remove_in_group}\")\n",
    "assert docs_to_remove_in_group == (len(largest_duplicate_cluster) - 1)  # noqa: S101"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365a6829-3566-4558-8b18-1fd5d0191d40",
   "metadata": {},
   "source": [
    "#### Advanced: Looking at examples of duplicate documents\n",
    "\n",
    "1. This analysis involves re-reading the input data with the same ID mapping that was used during duplicate identification.\n",
    "2. Merging the input data with the connected components results on the `_curator_dedup_id` column to associate each document which the duplicate group it belongs to which can be used for further analysis.\n",
    "\n",
    "**NOTE**: This analysis approach is intended as an example for smaller datasets and only works for cases where the connected components dataframe is small and fits comfortable in memory. It is not recommended for larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ef243e5-b8df-47d5-b993-fa61a3954eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.pipeline import Pipeline\n",
    "from nemo_curator.stages.base import ProcessingStage\n",
    "from nemo_curator.stages.resources import Resources\n",
    "from nemo_curator.stages.text.io.reader import JsonlReader, ParquetReader\n",
    "from nemo_curator.tasks.document import DocumentBatch\n",
    "\n",
    "\n",
    "class CustomMergeStage(ProcessingStage[DocumentBatch, DocumentBatch]):\n",
    "    \"\"\"\n",
    "    Warning: This should not be attempted with large connected components results.\n",
    "    A small stage that merges the input data (using the id's generated) with the connected components result.\n",
    "    Works because CC results are small enough to fit per batch.\n",
    "    \"\"\"\n",
    "\n",
    "    resources = Resources(cpus=1.0)\n",
    "\n",
    "    def process(self, batch: DocumentBatch) -> DocumentBatch:\n",
    "        df = batch.to_pandas().merge(cc_df, how=\"inner\", on=[CURATOR_DEDUP_ID_STR])\n",
    "        return DocumentBatch(\n",
    "            task_id=batch.task_id, dataset_name=batch.dataset_name, data=df, _stage_perf=batch._stage_perf\n",
    "        )\n",
    "\n",
    "\n",
    "ReaderClass = ParquetReader if input_filetype == \"parquet\" else JsonlReader\n",
    "pipeline = Pipeline(\n",
    "    name=\"Explore duplicates\",\n",
    "    stages=[\n",
    "        ReaderClass(file_paths=input_dataset_path, blocksize=input_blocksize, _assign_ids=True, read_kwargs=io_kwargs),\n",
    "        CustomMergeStage(),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44018e57-6748-4c39-aaf6-5caa2d1df247",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-15 23:13:58,592\tINFO worker.py:1696 -- Using address 127.0.1.1:6380 set in the environment variable RAY_ADDRESS\n",
      "2025-12-15 23:13:58,597\tINFO worker.py:1837 -- Connecting to existing Ray cluster at address: 127.0.1.1:6380...\n",
      "2025-12-15 23:13:58,597\tINFO worker.py:1855 -- Calling ray.init() again after it has already been called.\n",
      "2025-12-15 23:13:59,704\tINFO worker.py:1696 -- Using address 127.0.1.1:6380 set in the environment variable RAY_ADDRESS\n",
      "2025-12-15 23:13:59,709\tINFO worker.py:1837 -- Connecting to existing Ray cluster at address: 127.0.1.1:6380...\n",
      "2025-12-15 23:13:59,716\tINFO worker.py:2014 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8267 \u001b[39m\u001b[22m\n",
      "2025-12-15 23:13:59,736\tINFO worker.py:1696 -- Using address 127.0.1.1:6380 set in the environment variable RAY_ADDRESS\n",
      "2025-12-15 23:13:59,740\tINFO worker.py:1837 -- Connecting to existing Ray cluster at address: 127.0.1.1:6380...\n",
      "2025-12-15 23:13:59,740\tINFO worker.py:1855 -- Calling ray.init() again after it has already been called.\n",
      "2025-12-15 23:14:24,695\tINFO worker.py:1696 -- Using address 127.0.1.1:6380 set in the environment variable RAY_ADDRESS\n",
      "2025-12-15 23:14:24,699\tINFO worker.py:1837 -- Connecting to existing Ray cluster at address: 127.0.1.1:6380...\n",
      "2025-12-15 23:14:24,706\tINFO worker.py:2014 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8267 \u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "from nemo_curator.stages.deduplication.id_generator import create_id_generator_actor, kill_id_generator_actor\n",
    "\n",
    "try:\n",
    "    create_id_generator_actor(\n",
    "        filepath=os.path.join(fuzzy_output_dir, \"fuzzy_id_generator.json\"), storage_options=storage_options\n",
    "    )\n",
    "    merged_results = pipeline.run()\n",
    "    merged_df = pd.concat([batch.to_pandas() for batch in merged_results]).sort_values(\"_duplicate_group_id\")\n",
    "finally:\n",
    "    kill_id_generator_actor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "070d42e8-28c2-4abd-a40f-a0fe103befbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>_curator_dedup_id</th>\n",
       "      <th>_duplicate_group_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34010</th>\n",
       "      <td></td>\n",
       "      <td>92d6e01b-3292-494a-b139-9479bdb6e624</td>\n",
       "      <td>115098</td>\n",
       "      <td>14100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176822</th>\n",
       "      <td></td>\n",
       "      <td>cbcc728f-089d-4e63-99d4-a23f35736955</td>\n",
       "      <td>610909</td>\n",
       "      <td>14100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176823</th>\n",
       "      <td></td>\n",
       "      <td>830bd2ba-0f07-4e89-ae60-ba14d0a2fa09</td>\n",
       "      <td>610910</td>\n",
       "      <td>14100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94578</th>\n",
       "      <td></td>\n",
       "      <td>7cab3473-0f8e-4b8b-a22e-6961151572d0</td>\n",
       "      <td>327273</td>\n",
       "      <td>14100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176824</th>\n",
       "      <td></td>\n",
       "      <td>db416c14-9ce3-4ac8-87a6-3ea546ddb1bc</td>\n",
       "      <td>610912</td>\n",
       "      <td>14100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28991</th>\n",
       "      <td></td>\n",
       "      <td>c2c4f14a-f405-472c-aece-e36a2c4a8762</td>\n",
       "      <td>98886</td>\n",
       "      <td>14100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57528</th>\n",
       "      <td></td>\n",
       "      <td>37173cac-340e-4a27-9e22-d0fb23bd0aee</td>\n",
       "      <td>1188225</td>\n",
       "      <td>14100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28988</th>\n",
       "      <td></td>\n",
       "      <td>2dab220a-bd13-4b1a-ba1c-f0459b16c268</td>\n",
       "      <td>98880</td>\n",
       "      <td>14100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28990</th>\n",
       "      <td></td>\n",
       "      <td>22ec3467-721d-4f9d-a0c1-3d654d909ae7</td>\n",
       "      <td>98884</td>\n",
       "      <td>14100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28989</th>\n",
       "      <td></td>\n",
       "      <td>c6dc1361-5250-4972-be2f-1dd44e4e3f9b</td>\n",
       "      <td>98882</td>\n",
       "      <td>14100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>230 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       text                                    id  _curator_dedup_id  \\\n",
       "34010        92d6e01b-3292-494a-b139-9479bdb6e624             115098   \n",
       "176822       cbcc728f-089d-4e63-99d4-a23f35736955             610909   \n",
       "176823       830bd2ba-0f07-4e89-ae60-ba14d0a2fa09             610910   \n",
       "94578        7cab3473-0f8e-4b8b-a22e-6961151572d0             327273   \n",
       "176824       db416c14-9ce3-4ac8-87a6-3ea546ddb1bc             610912   \n",
       "...     ...                                   ...                ...   \n",
       "28991        c2c4f14a-f405-472c-aece-e36a2c4a8762              98886   \n",
       "57528        37173cac-340e-4a27-9e22-d0fb23bd0aee            1188225   \n",
       "28988        2dab220a-bd13-4b1a-ba1c-f0459b16c268              98880   \n",
       "28990        22ec3467-721d-4f9d-a0c1-3d654d909ae7              98884   \n",
       "28989        c6dc1361-5250-4972-be2f-1dd44e4e3f9b              98882   \n",
       "\n",
       "        _duplicate_group_id  \n",
       "34010                 14100  \n",
       "176822                14100  \n",
       "176823                14100  \n",
       "94578                 14100  \n",
       "176824                14100  \n",
       "...                     ...  \n",
       "28991                 14100  \n",
       "57528                 14100  \n",
       "28988                 14100  \n",
       "28990                 14100  \n",
       "28989                 14100  \n",
       "\n",
       "[230 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(merged_df[merged_df._curator_dedup_id.isin(largest_duplicate_cluster)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc423318-c1d5-4769-ba42-362f515c3166",
   "metadata": {},
   "source": [
    "The largest cluster/group of duplicates in this dataset seems to be all documents with empty/no text.\n",
    "\n",
    "Let's look at the second largest cluster of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a08dc31e-d4f9-47e0-b8b7-7f75e4e6abef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>_curator_dedup_id</th>\n",
       "      <th>_duplicate_group_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>300414</th>\n",
       "      <td>Sara and Ben were friends who liked to play in...</td>\n",
       "      <td>dccdcb21-13c0-4df9-b4eb-cf6871f57969</td>\n",
       "      <td>1994745</td>\n",
       "      <td>153774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106660</th>\n",
       "      <td>Sara and Ben were friends who liked to play in...</td>\n",
       "      <td>2012a26a-c0fb-448d-be3a-955a5fb8f165</td>\n",
       "      <td>373063</td>\n",
       "      <td>153774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209932</th>\n",
       "      <td>Sara and Ben were friends who liked to play in...</td>\n",
       "      <td>296aad0d-3fcc-4bcd-9852-2d7907f715af</td>\n",
       "      <td>1698008</td>\n",
       "      <td>153774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  \\\n",
       "300414  Sara and Ben were friends who liked to play in...   \n",
       "106660  Sara and Ben were friends who liked to play in...   \n",
       "209932  Sara and Ben were friends who liked to play in...   \n",
       "\n",
       "                                          id  _curator_dedup_id  \\\n",
       "300414  dccdcb21-13c0-4df9-b4eb-cf6871f57969            1994745   \n",
       "106660  2012a26a-c0fb-448d-be3a-955a5fb8f165             373063   \n",
       "209932  296aad0d-3fcc-4bcd-9852-2d7907f715af            1698008   \n",
       "\n",
       "        _duplicate_group_id  \n",
       "300414               153774  \n",
       "106660               153774  \n",
       "209932               153774  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document1\n",
      "----------\n",
      "Sara and Ben were friends who liked to play in the park. One day, they saw a big dog with a red bow on its neck. Sara wanted to pet the dog, but Ben was scared.\n",
      "\n",
      "\"Come on, Ben, the dog is nice. Look, it has a bow. It wants to be our friend,\" Sara said.\n",
      "\n",
      "\"No, Sara, the dog is big and loud. It might bite us. We should go away,\" Ben said.\n",
      "\n",
      "Sara did not listen to Ben. She ran to the dog and tried to touch its bow. The dog did not like that. It growled and barked at Sara. It showed its teeth and snapped at her hand. Sara was scared and ran back to Ben.\n",
      "\n",
      "\"Are you okay, Sara?\" Ben asked.\n",
      "\n",
      "\"Yes, Ben, I am okay. But the dog was terrible. It did not want me to pet it. It was mean to me,\" Sara said.\n",
      "\n",
      "\"I told you, Sara, the dog was big and loud. You should have listened to me. We should not bother animals we do not know. They might hurt us,\" Ben said.\n",
      "\n",
      "Sara nodded. She was sorry she did not listen to Ben. She learned her lesson. She and Ben went to play with their own toys. They were happy and safe.\n",
      "\n",
      "The moral of the story is: Listen to your friends when they warn you about danger. Do not bother animals you do not know. They might hurt you.\n",
      "\n",
      "Document2\n",
      "----------\n",
      "Sara and Ben were friends who liked to play in the park. One day, they saw a big dog with a red bow on its neck. Sara wanted to pet the dog, but Ben was scared.\n",
      "\n",
      "\"Come on, Ben, the dog is nice. Look, it has a bow. It wants to be our friend,\" Sara said.\n",
      "\n",
      "\"No, Sara, the dog is big and loud. It might bite us. We should go away,\" Ben said.\n",
      "\n",
      "Sara did not listen to Ben. She ran to the dog and tried to touch its bow. The dog did not like that. It growled and barked at Sara. It showed its teeth and snapped at her hand. Sara was scared and ran back to Ben.\n",
      "\n",
      "\"Are you okay, Sara?\" Ben asked.\n",
      "\n",
      "\"Yes, Ben, I am okay. But the dog was terrible. It did not want me to pet it. It was mean to me,\" Sara said.\n",
      "\n",
      "\"I told you, Sara, the dog was big and loud. You should have listened to me. We should not bother animals we do not know. They might hurt us,\" Ben said.\n",
      "\n",
      "Sara nodded. She was sorry she did not listen to Ben. She learned her lesson. She and Ben went to play with their own toys. They were happy and safe.\n",
      "\n",
      "The moral of the story is: Listen to your friends when they warn you about danger. Do not bother animals you do not know. They might hurt you.\n"
     ]
    }
   ],
   "source": [
    "duplicates = merged_df[merged_df._curator_dedup_id.isin(grouped_cc_df.loc[duplicate_cluster_sizes.index[1]])]\n",
    "display(duplicates)\n",
    "\n",
    "print(f\"\\nDocument1\\n----------\\n{duplicates.iloc[0].text}\")\n",
    "print(f\"\\nDocument2\\n----------\\n{duplicates.iloc[1].text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be9aca1d-f26a-4767-a9bc-9019969da01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3016ea8c-26b0-451b-b436-09ebdef9a5df",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "We were able to find and remove ~320_000 duplicate documents in a dataset of ~2.1 Million Rows "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
