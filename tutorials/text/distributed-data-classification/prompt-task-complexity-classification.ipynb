{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Data Classification with NeMo Curator's `PromptTaskComplexityClassifier`\n",
    "\n",
    "This notebook demonstrates the use of NeMo Curator's `PromptTaskComplexityClassifier`. The [prompt task and complexity classifier](https://huggingface.co/nvidia/prompt-task-and-complexity-classifier) a multi-headed model which classifies English text prompts across task types and complexity dimensions. It helps with data annotation, which is useful in data blending for foundation model training. Please refer to the NemoCurator Prompt Task and Complexity Classifier Hugging Face page for more information about the prompt task and complexity classifier, including its output labels, here: https://huggingface.co/nvidia/prompt-task-and-complexity-classifier.\n",
    "\n",
    "This tutorial requires at least 1 NVIDIA GPU with:\n",
    "  - Volta™ or higher (compute capability 7.0+)\n",
    "  - CUDA 12.x\n",
    "\n",
    "For more information about the classifiers, refer to our [Distributed Data Classification](https://docs.nvidia.com/nemo/curator/latest/curate-text/process-data/quality-assessment/distributed-classifier.html) documentation page.\n",
    "\n",
    "Before running this notebook, see this [Installation Guide](https://docs.nvidia.com/nemo/curator/latest/admin/installation.html#admin-installation) page for instructions on how to install NeMo Curator. Be sure to use an installation method which includes GPU dependencies (`text_cuda12` or `all`). Check proper installation with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, silence Curator logs via Loguru\n",
    "import os\n",
    "\n",
    "os.environ[\"LOGURU_LEVEL\"] = \"ERROR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nemo_curator\n",
    "\n",
    "nemo_curator.__version__  # should be >= 1.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that GPUs are available, then check that the `gpustat` dependency was installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpustat\n",
    "\n",
    "gpustat.__version__  # check gpu dependency is installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following imports are required for this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from nemo_curator.core.client import RayClient\n",
    "from nemo_curator.pipeline import Pipeline\n",
    "from nemo_curator.stages.text.classifiers import PromptTaskComplexityClassifier\n",
    "from nemo_curator.stages.text.io.reader.jsonl import JsonlReader\n",
    "from nemo_curator.stages.text.io.reader.parquet import ParquetReader\n",
    "from nemo_curator.stages.text.io.writer.jsonl import JsonlWriter\n",
    "from nemo_curator.stages.text.io.writer.parquet import ParquetWriter\n",
    "from nemo_curator.utils.file_utils import get_all_file_paths_under"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run a pipeline in NeMo Curator, we must start a Ray cluster. This can be done manually (see the [Ray documentation](https://docs.ray.io/en/latest/ray-core/starting-ray.html)) or with Curator's `RayClient`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ray_client = RayClient()\n",
    "    ray_client.start()\n",
    "except Exception as e:\n",
    "    msg = f\"Error initializing Ray client: {e}\"\n",
    "    raise RuntimeError(msg) from e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Read, Classification, and Write Stages\n",
    "\n",
    "Functions in NeMo Curator are called stages. For this tutorial, we will initialize 3 stages: a file reader, a prompt task and complexity classification stage, and a file writer.\n",
    "\n",
    "For this tutorial, an existing directory containing JSONL or Parquet files may be provided via `input_file_path`. If `input_file_path` does not exist or is empty, we download a small dataset from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = \"./prompt_data_dir\"\n",
    "input_file_type = \"jsonl\"  # can be \"jsonl\" or \"parquet\"\n",
    "text_field = \"instruction\"\n",
    "\n",
    "if len(get_all_file_paths_under(input_file_path)) == 0:\n",
    "    from huggingface_hub import snapshot_download\n",
    "\n",
    "    # Download JSONL file from Hugging Face\n",
    "    # This dataset contains only English texts\n",
    "    snapshot_download(\n",
    "        repo_id=\"databricks/databricks-dolly-15k\",\n",
    "        repo_type=\"dataset\",\n",
    "        local_dir=input_file_path,\n",
    "    )\n",
    "    print(f\"Downloaded Hugging Face dataset to {input_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define the reader stage with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read directory of files and only read the text field\n",
    "if input_file_type == \"jsonl\":\n",
    "    read_stage = JsonlReader(input_file_path, files_per_partition=1, fields=[text_field])\n",
    "elif input_file_type == \"parquet\":\n",
    "    read_stage = ParquetReader(input_file_path, files_per_partition=1, fields=[text_field])\n",
    "else:\n",
    "    msg = f\"Invalid input file type: {input_file_type}\"\n",
    "    raise ValueError(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier stage is broken down under the hood into a tokenizer stage and a model inference stage. Tokenization is run on the CPU while model inference is run on the GPU. This means that behind the scenes, the `PromptTaskComplexityClassifier` stage is actually being broken down into 2 stages (some parameters and details omitted to avoid complexity, please refer to the documentation for more details):\n",
    "\n",
    "```python\n",
    "class TokenizerStage:\n",
    "    self.resources = Resources(cpus=1)\n",
    "    self.model_identifier = \"nvidia/prompt-task-and-complexity-classifier\"\n",
    "    self.text_field = \"text\"\n",
    "    self.padding_side = \"right\"\n",
    "    ...\n",
    "class ModelStage:\n",
    "    self.resources = Resources(cpus=1, gpus=1)\n",
    "    self.model_identifier = \"nvidia/prompt-task-and-complexity-classifier\"\n",
    "    self.model_inference_batch_size = 256\n",
    "    ...\n",
    "```\n",
    "\n",
    "See the [API Reference](https://docs.nvidia.com/nemo/curator/latest/apidocs/stages/stages.text.classifiers.prompt_task_complexity.html#stages.text.classifiers.prompt_task_complexity.PromptTaskComplexityClassifier) for more information about the `PromptTaskComplexityClassifier` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the prompt task and complexity classifier\n",
    "classifier_stage = PromptTaskComplexityClassifier(text_field=text_field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can define a stage for writing the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = \"./prompt_task_complexity_classifier_results\"\n",
    "output_file_type = \"jsonl\"  # can be \"jsonl\" or \"parquet\"\n",
    "\n",
    "# Use mode=\"overwrite\" to overwrite the output directory if it already exists\n",
    "# This helps to ensure that the correct output is written\n",
    "if output_file_type == \"jsonl\":\n",
    "    write_stage = JsonlWriter(output_file_path, mode=\"overwrite\")\n",
    "elif output_file_type == \"parquet\":\n",
    "    write_stage = ParquetWriter(output_file_path, mode=\"overwrite\")\n",
    "else:\n",
    "    msg = f\"Invalid output file type: {output_file_type}\"\n",
    "    raise ValueError(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Pipeline\n",
    "\n",
    "In NeMo Curator, we use pipelines to run distributed data workflows using Ray. Pipelines take care of resource allocation and autoscaling to achieve enhanced performance and minimize GPU idleness.\n",
    "\n",
    "For the distributed data classifiers, we are able to achieve speedups by ensuring that model inference is run in parallel across all available GPUs, while other stages such as I/O, tokenization, and filtering are run across all available CPUs. This is possible because Curator pipelines are composable, which allows each stage in a pipeline to run independently and with its own specified hardware resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(name='classifier_pipeline', stages=[jsonl_reader(JsonlReader), prompt_task_and_complexity_classifier_classifier(PromptTaskComplexityClassifier), jsonl_writer(JsonlWriter)])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_pipeline = Pipeline(name=\"classifier_pipeline\", description=\"Run a classifier pipeline\")\n",
    "\n",
    "# Add stages to the pipeline\n",
    "classifier_pipeline.add_stage(read_stage)\n",
    "classifier_pipeline.add_stage(classifier_stage)\n",
    "classifier_pipeline.add_stage(write_stage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Composability is also what allows a classifier to sit between pre-processing and post-processing stages. Typical text pre-processing add-ons include text normalization (lowercasing, URL/email removal, Unicode cleanup) and language identification and filtering (to keep only target languages). A full pipeline may look something like:\n",
    "\n",
    "```python\n",
    "pipeline = Pipeline(name=\"full_pipeline\")\n",
    "pipeline.add_stage(read_stage)                # reader (JSONL/Parquet)\n",
    "pipeline.add_stage(lang_id_stage)             # optional: language filter\n",
    "pipeline.add_stage(classifier_stage)          # classifier\n",
    "pipeline.add_stage(write_stage)               # writer (JSONL/Parquet)\n",
    "```\n",
    "\n",
    "# Run the  Classifier\n",
    "\n",
    "Let's run the full classifier pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 15:10:33,841\tINFO worker.py:1692 -- Using address 127.0.1.1:6380 set in the environment variable RAY_ADDRESS\n",
      "2025-12-03 15:10:33,844\tINFO worker.py:1833 -- Connecting to existing Ray cluster at address: 127.0.1.1:6380...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-03 15:10:33,774\tINFO usage_lib.py:447 -- Usage stats collection is disabled.\n",
      "2025-12-03 15:10:33,775\tINFO scripts.py:914 -- \u001b[37mLocal node IP\u001b[39m: \u001b[1m127.0.1.1\u001b[22m\n",
      "2025-12-03 15:10:35,767\tSUCC scripts.py:950 -- \u001b[32m--------------------\u001b[39m\n",
      "2025-12-03 15:10:35,767\tSUCC scripts.py:951 -- \u001b[32mRay runtime started.\u001b[39m\n",
      "2025-12-03 15:10:35,767\tSUCC scripts.py:952 -- \u001b[32m--------------------\u001b[39m\n",
      "2025-12-03 15:10:35,767\tINFO scripts.py:954 -- \u001b[36mNext steps\u001b[39m\n",
      "2025-12-03 15:10:35,767\tINFO scripts.py:957 -- To add another node to this Ray cluster, run\n",
      "2025-12-03 15:10:35,767\tINFO scripts.py:960 -- \u001b[1m  ray start --address='127.0.1.1:6380'\u001b[22m\n",
      "2025-12-03 15:10:35,768\tINFO scripts.py:969 -- To connect to this Ray cluster:\n",
      "2025-12-03 15:10:35,768\tINFO scripts.py:971 -- \u001b[35mimport\u001b[39m\u001b[26m ray\n",
      "2025-12-03 15:10:35,768\tINFO scripts.py:972 -- ray\u001b[35m.\u001b[39m\u001b[26minit(_node_ip_address\u001b[35m=\u001b[39m\u001b[26m\u001b[33m'127.0.1.1'\u001b[39m\u001b[26m)\n",
      "2025-12-03 15:10:35,768\tINFO scripts.py:984 -- To submit a Ray job using the Ray Jobs CLI:\n",
      "2025-12-03 15:10:35,768\tINFO scripts.py:985 -- \u001b[1m  RAY_API_SERVER_ADDRESS='http://127.0.0.1:8266' ray job submit --working-dir . -- python my_script.py\u001b[22m\n",
      "2025-12-03 15:10:35,768\tINFO scripts.py:994 -- See https://docs.ray.io/en/latest/cluster/running-applications/job-submission/index.html \n",
      "2025-12-03 15:10:35,768\tINFO scripts.py:998 -- for more information on submitting Ray jobs to the Ray cluster.\n",
      "2025-12-03 15:10:35,768\tINFO scripts.py:1003 -- To terminate the Ray runtime, run\n",
      "2025-12-03 15:10:35,768\tINFO scripts.py:1004 -- \u001b[1m  ray stop\u001b[22m\n",
      "2025-12-03 15:10:35,768\tINFO scripts.py:1007 -- To view the status of the cluster, use\n",
      "2025-12-03 15:10:35,768\tINFO scripts.py:1008 --   \u001b[1mray status\u001b[22m\u001b[26m\n",
      "2025-12-03 15:10:35,768\tINFO scripts.py:1012 -- To monitor and debug Ray, view the dashboard at \n",
      "2025-12-03 15:10:35,768\tINFO scripts.py:1013 --   \u001b[1m127.0.0.1:8266\u001b[22m\u001b[26m\n",
      "2025-12-03 15:10:35,768\tINFO scripts.py:1020 -- \u001b[4mIf connection to the dashboard fails, check your firewall settings and network configuration.\u001b[24m\n",
      "2025-12-03 15:10:35,769\tINFO scripts.py:1121 -- \u001b[36m\u001b[1m--block\u001b[22m\u001b[39m\n",
      "2025-12-03 15:10:35,769\tINFO scripts.py:1122 -- This command will now block forever until terminated by a signal.\n",
      "2025-12-03 15:10:35,769\tINFO scripts.py:1125 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 15:10:36,164\tINFO worker.py:2004 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8266 \u001b[39m\u001b[22m\n",
      "/raid/syurick/conda/ray-curator/lib/python3.12/site-packages/ray/_private/worker.py:2052: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
      "  warnings.warn(\n",
      "2025-12-03 15:10:37,103\tINFO worker.py:1692 -- Using address 127.0.1.1:6380 set in the environment variable RAY_ADDRESS\n",
      "2025-12-03 15:10:37,105\tINFO worker.py:1833 -- Connecting to existing Ray cluster at address: 127.0.1.1:6380...\n",
      "2025-12-03 15:10:37,106\tINFO worker.py:1851 -- Calling ray.init() again after it has already been called.\n",
      "Fetching 10 files: 100%|██████████| 10/10 [00:00<00:00, 25481.80it/s]\n",
      "Fetching 10 files: 100%|██████████| 10/10 [00:00<00:00, 48657.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# Run the pipeline\n",
    "result = classifier_pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the pipeline ran to completion and the result was written to disk, we can shut down the Ray cluster with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ray_client.stop()\n",
    "except Exception as e:  # noqa: BLE001\n",
    "    print(f\"Error stopping Ray client: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect the Output\n",
    "\n",
    "The write stage returns a list of written files. We can read the output file as a Pandas DataFrame for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>prompt_complexity_score</th>\n",
       "      <th>task_type_1</th>\n",
       "      <th>task_type_2</th>\n",
       "      <th>task_type_prob</th>\n",
       "      <th>creativity_scope</th>\n",
       "      <th>reasoning</th>\n",
       "      <th>contextual_knowledge</th>\n",
       "      <th>number_of_few_shots</th>\n",
       "      <th>domain_knowledge</th>\n",
       "      <th>no_label_reason</th>\n",
       "      <th>constraint_ct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When did Virgin Australia start operating?</td>\n",
       "      <td>0.06551</td>\n",
       "      <td>Open QA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.1245</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3718</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which is a species of fish? Tope or Rope</td>\n",
       "      <td>0.06294</td>\n",
       "      <td>Open QA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.0122</td>\n",
       "      <td>0.0080</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3603</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why can camels survive for long without water?</td>\n",
       "      <td>0.09135</td>\n",
       "      <td>Open QA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0.988</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0318</td>\n",
       "      <td>0.0472</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5091</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alice's parents have three daughters: Amy, Jes...</td>\n",
       "      <td>0.12010</td>\n",
       "      <td>Open QA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0.939</td>\n",
       "      <td>0.0245</td>\n",
       "      <td>0.2276</td>\n",
       "      <td>0.1870</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2698</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When was Tomoaki Komorida born?</td>\n",
       "      <td>0.06714</td>\n",
       "      <td>Open QA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.1187</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3877</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         instruction  prompt_complexity_score  \\\n",
       "0         When did Virgin Australia start operating?                  0.06551   \n",
       "1           Which is a species of fish? Tope or Rope                  0.06294   \n",
       "2     Why can camels survive for long without water?                  0.09135   \n",
       "3  Alice's parents have three daughters: Amy, Jes...                  0.12010   \n",
       "4                    When was Tomoaki Komorida born?                  0.06714   \n",
       "\n",
       "  task_type_1 task_type_2  task_type_prob  creativity_scope  reasoning  \\\n",
       "0     Open QA          NA           0.989            0.0033     0.0047   \n",
       "1     Open QA          NA           0.973            0.0122     0.0080   \n",
       "2     Open QA          NA           0.988            0.0065     0.0318   \n",
       "3     Open QA          NA           0.939            0.0245     0.2276   \n",
       "4     Open QA          NA           0.989            0.0036     0.0035   \n",
       "\n",
       "   contextual_knowledge  number_of_few_shots  domain_knowledge  \\\n",
       "0                0.1245                  0.0            0.3718   \n",
       "1                0.0105                  0.0            0.3603   \n",
       "2                0.0472                  0.0            0.5091   \n",
       "3                0.1870                  0.0            0.2698   \n",
       "4                0.1187                  0.0            0.3877   \n",
       "\n",
       "   no_label_reason  constraint_ct  \n",
       "0                0         0.0079  \n",
       "1                0         0.0140  \n",
       "2                0         0.0160  \n",
       "3                0         0.0320  \n",
       "4                0         0.0061  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For simplicity, we take the first written file from the writer stage\n",
    "# In real pipelines, adjust as needed\n",
    "result_file = result[0].data[0]\n",
    "\n",
    "result_df = pd.read_json(result_file, lines=True) if output_file_type == \"jsonl\" else pd.read_parquet(result_file)\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the distribution of predictions for this file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "task_type_1\n",
       "Open QA            12801\n",
       "Classification       649\n",
       "Text Generation      625\n",
       "Brainstorming        398\n",
       "Extraction           203\n",
       "Rewrite              122\n",
       "Closed QA             86\n",
       "Summarization         70\n",
       "Code Generation       40\n",
       "Other                 14\n",
       "Chatbot                3\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df[\"task_type_1\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the predictions were generated as expected. For more information about the ranges of the output values for the prompt task and complexity classifier, please refer to the [Hugging Face page](https://huggingface.co/nvidia/prompt-task-and-complexity-classifier)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
